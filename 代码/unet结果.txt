__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to
==================================================================================================
input_1 (InputLayer)            (None, 256, 256, 1)  0
__________________________________________________________________________________________________
conv2d (Conv2D)                 (None, 256, 256, 16) 160         input_1[0][0]
__________________________________________________________________________________________________
batch_normalization (BatchNorma (None, 256, 256, 16) 64          conv2d[0][0]
__________________________________________________________________________________________________
activation (Activation)         (None, 256, 256, 16) 0           batch_normalization[0][0]        
__________________________________________________________________________________________________
conv2d_1 (Conv2D)               (None, 256, 256, 16) 2320        activation[0][0]
__________________________________________________________________________________________________
batch_normalization_1 (BatchNor (None, 256, 256, 16) 64          conv2d_1[0][0]
__________________________________________________________________________________________________
activation_1 (Activation)       (None, 256, 256, 16) 0           batch_normalization_1[0][0]      
__________________________________________________________________________________________________
max_pooling2d (MaxPooling2D)    (None, 128, 128, 16) 0           activation_1[0][0]
__________________________________________________________________________________________________
dropout (Dropout)               (None, 128, 128, 16) 0           max_pooling2d[0][0]
__________________________________________________________________________________________________
conv2d_2 (Conv2D)               (None, 128, 128, 32) 4640        dropout[0][0]
__________________________________________________________________________________________________
batch_normalization_2 (BatchNor (None, 128, 128, 32) 128         conv2d_2[0][0]
__________________________________________________________________________________________________
activation_2 (Activation)       (None, 128, 128, 32) 0           batch_normalization_2[0][0]      
__________________________________________________________________________________________________
conv2d_3 (Conv2D)               (None, 128, 128, 32) 9248        activation_2[0][0]
__________________________________________________________________________________________________
batch_normalization_3 (BatchNor (None, 128, 128, 32) 128         conv2d_3[0][0]
__________________________________________________________________________________________________
activation_3 (Activation)       (None, 128, 128, 32) 0           batch_normalization_3[0][0]
__________________________________________________________________________________________________
max_pooling2d_1 (MaxPooling2D)  (None, 64, 64, 32)   0           activation_3[0][0]
__________________________________________________________________________________________________
dropout_1 (Dropout)             (None, 64, 64, 32)   0           max_pooling2d_1[0][0]
__________________________________________________________________________________________________
conv2d_4 (Conv2D)               (None, 64, 64, 64)   18496       dropout_1[0][0]
__________________________________________________________________________________________________
batch_normalization_4 (BatchNor (None, 64, 64, 64)   256         conv2d_4[0][0]
__________________________________________________________________________________________________
activation_4 (Activation)       (None, 64, 64, 64)   0           batch_normalization_4[0][0]
__________________________________________________________________________________________________
conv2d_5 (Conv2D)               (None, 64, 64, 64)   36928       activation_4[0][0]
__________________________________________________________________________________________________
batch_normalization_5 (BatchNor (None, 64, 64, 64)   256         conv2d_5[0][0]
__________________________________________________________________________________________________
activation_5 (Activation)       (None, 64, 64, 64)   0           batch_normalization_5[0][0]
__________________________________________________________________________________________________
max_pooling2d_2 (MaxPooling2D)  (None, 32, 32, 64)   0           activation_5[0][0]
__________________________________________________________________________________________________
dropout_2 (Dropout)             (None, 32, 32, 64)   0           max_pooling2d_2[0][0]
__________________________________________________________________________________________________
conv2d_6 (Conv2D)               (None, 32, 32, 128)  73856       dropout_2[0][0]
__________________________________________________________________________________________________
batch_normalization_6 (BatchNor (None, 32, 32, 128)  512         conv2d_6[0][0]
__________________________________________________________________________________________________
activation_6 (Activation)       (None, 32, 32, 128)  0           batch_normalization_6[0][0]
__________________________________________________________________________________________________
conv2d_7 (Conv2D)               (None, 32, 32, 128)  147584      activation_6[0][0]
__________________________________________________________________________________________________
batch_normalization_7 (BatchNor (None, 32, 32, 128)  512         conv2d_7[0][0]
__________________________________________________________________________________________________
activation_7 (Activation)       (None, 32, 32, 128)  0           batch_normalization_7[0][0]
__________________________________________________________________________________________________
max_pooling2d_3 (MaxPooling2D)  (None, 16, 16, 128)  0           activation_7[0][0]
__________________________________________________________________________________________________
dropout_3 (Dropout)             (None, 16, 16, 128)  0           max_pooling2d_3[0][0]
__________________________________________________________________________________________________
conv2d_8 (Conv2D)               (None, 16, 16, 256)  295168      dropout_3[0][0]
__________________________________________________________________________________________________
batch_normalization_8 (BatchNor (None, 16, 16, 256)  1024        conv2d_8[0][0]
__________________________________________________________________________________________________
activation_8 (Activation)       (None, 16, 16, 256)  0           batch_normalization_8[0][0]
__________________________________________________________________________________________________
conv2d_9 (Conv2D)               (None, 16, 16, 256)  590080      activation_8[0][0]
__________________________________________________________________________________________________
batch_normalization_9 (BatchNor (None, 16, 16, 256)  1024        conv2d_9[0][0]
__________________________________________________________________________________________________
activation_9 (Activation)       (None, 16, 16, 256)  0           batch_normalization_9[0][0]
__________________________________________________________________________________________________
conv2d_transpose (Conv2DTranspo (None, 32, 32, 128)  295040      activation_9[0][0]
__________________________________________________________________________________________________
concatenate (Concatenate)       (None, 32, 32, 256)  0           conv2d_transpose[0][0]
                                                                 activation_7[0][0]
__________________________________________________________________________________________________
dropout_4 (Dropout)             (None, 32, 32, 256)  0           concatenate[0][0]
__________________________________________________________________________________________________
conv2d_10 (Conv2D)              (None, 32, 32, 128)  295040      dropout_4[0][0]
__________________________________________________________________________________________________
batch_normalization_10 (BatchNo (None, 32, 32, 128)  512         conv2d_10[0][0]
__________________________________________________________________________________________________
activation_10 (Activation)      (None, 32, 32, 128)  0           batch_normalization_10[0][0]
__________________________________________________________________________________________________
conv2d_11 (Conv2D)              (None, 32, 32, 128)  147584      activation_10[0][0]
__________________________________________________________________________________________________
batch_normalization_11 (BatchNo (None, 32, 32, 128)  512         conv2d_11[0][0]
__________________________________________________________________________________________________
activation_11 (Activation)      (None, 32, 32, 128)  0           batch_normalization_11[0][0]
__________________________________________________________________________________________________
conv2d_transpose_1 (Conv2DTrans (None, 64, 64, 64)   73792       activation_11[0][0]
__________________________________________________________________________________________________
concatenate_1 (Concatenate)     (None, 64, 64, 128)  0           conv2d_transpose_1[0][0]
                                                                 activation_5[0][0]
__________________________________________________________________________________________________
dropout_5 (Dropout)             (None, 64, 64, 128)  0           concatenate_1[0][0]
__________________________________________________________________________________________________
conv2d_12 (Conv2D)              (None, 64, 64, 64)   73792       dropout_5[0][0]
__________________________________________________________________________________________________
batch_normalization_12 (BatchNo (None, 64, 64, 64)   256         conv2d_12[0][0]
__________________________________________________________________________________________________
activation_12 (Activation)      (None, 64, 64, 64)   0           batch_normalization_12[0][0]
__________________________________________________________________________________________________
conv2d_13 (Conv2D)              (None, 64, 64, 64)   36928       activation_12[0][0]
__________________________________________________________________________________________________
batch_normalization_13 (BatchNo (None, 64, 64, 64)   256         conv2d_13[0][0]
__________________________________________________________________________________________________
activation_13 (Activation)      (None, 64, 64, 64)   0           batch_normalization_13[0][0]
__________________________________________________________________________________________________
conv2d_transpose_2 (Conv2DTrans (None, 128, 128, 32) 18464       activation_13[0][0]
__________________________________________________________________________________________________
concatenate_2 (Concatenate)     (None, 128, 128, 64) 0           conv2d_transpose_2[0][0]
                                                                 activation_3[0][0]
__________________________________________________________________________________________________
dropout_6 (Dropout)             (None, 128, 128, 64) 0           concatenate_2[0][0]
__________________________________________________________________________________________________
conv2d_14 (Conv2D)              (None, 128, 128, 32) 18464       dropout_6[0][0]
__________________________________________________________________________________________________
batch_normalization_14 (BatchNo (None, 128, 128, 32) 128         conv2d_14[0][0]
__________________________________________________________________________________________________
activation_14 (Activation)      (None, 128, 128, 32) 0           batch_normalization_14[0][0]
__________________________________________________________________________________________________
conv2d_15 (Conv2D)              (None, 128, 128, 32) 9248        activation_14[0][0]
__________________________________________________________________________________________________
batch_normalization_15 (BatchNo (None, 128, 128, 32) 128         conv2d_15[0][0]
__________________________________________________________________________________________________
activation_15 (Activation)      (None, 128, 128, 32) 0           batch_normalization_15[0][0]
__________________________________________________________________________________________________
conv2d_transpose_3 (Conv2DTrans (None, 256, 256, 16) 4624        activation_15[0][0]
__________________________________________________________________________________________________
concatenate_3 (Concatenate)     (None, 256, 256, 32) 0           conv2d_transpose_3[0][0]
                                                                 activation_1[0][0]
__________________________________________________________________________________________________
dropout_7 (Dropout)             (None, 256, 256, 32) 0           concatenate_3[0][0]
__________________________________________________________________________________________________
conv2d_16 (Conv2D)              (None, 256, 256, 16) 4624        dropout_7[0][0]
__________________________________________________________________________________________________
batch_normalization_16 (BatchNo (None, 256, 256, 16) 64          conv2d_16[0][0]
__________________________________________________________________________________________________
activation_16 (Activation)      (None, 256, 256, 16) 0           batch_normalization_16[0][0]
__________________________________________________________________________________________________
conv2d_17 (Conv2D)              (None, 256, 256, 16) 2320        activation_16[0][0]
__________________________________________________________________________________________________
batch_normalization_17 (BatchNo (None, 256, 256, 16) 64          conv2d_17[0][0]
__________________________________________________________________________________________________
activation_17 (Activation)      (None, 256, 256, 16) 0           batch_normalization_17[0][0]     
__________________________________________________________________________________________________
conv2d_18 (Conv2D)              (None, 256, 256, 1)  17          activation_17[0][0]
==================================================================================================
Total params: 2,164,305
Trainable params: 2,161,361
Non-trainable params: 2,944
__________________________________________________________________________________________________
Train on 2610 samples, validate on 290 samples
Epoch 1/101
 - 176s - loss: 0.0184 - metric_fun: 0.0942 - val_loss: 0.0074 - val_metric_fun: 7.3387e-05

Epoch 00001: val_loss improved from inf to 0.00741, saving model to ./weights/best_model.h5
Epoch 2/101
 - 141s - loss: 0.0033 - metric_fun: 0.2714 - val_loss: 0.0028 - val_metric_fun: 0.2935

Epoch 00002: val_loss improved from 0.00741 to 0.00279, saving model to ./weights/best_model.h5
Epoch 3/101
 - 141s - loss: 0.0025 - metric_fun: 0.3546 - val_loss: 0.0026 - val_metric_fun: 0.3794

Epoch 00003: val_loss improved from 0.00279 to 0.00256, saving model to ./weights/best_model.h5
Epoch 4/101
 - 141s - loss: 0.0020 - metric_fun: 0.4149 - val_loss: 0.0017 - val_metric_fun: 0.4546

Epoch 00004: val_loss improved from 0.00256 to 0.00170, saving model to ./weights/best_model.h5
Epoch 5/101
 - 141s - loss: 0.0018 - metric_fun: 0.4445 - val_loss: 0.0017 - val_metric_fun: 0.4465

Epoch 00005: val_loss did not improve from 0.00170
Epoch 6/101
 - 141s - loss: 0.0018 - metric_fun: 0.4238 - val_loss: 0.0017 - val_metric_fun: 0.5460

Epoch 00006: val_loss did not improve from 0.00170
Epoch 7/101
 - 141s - loss: 0.0015 - metric_fun: 0.4876 - val_loss: 0.0015 - val_metric_fun: 0.4663

Epoch 00007: val_loss improved from 0.00170 to 0.00146, saving model to ./weights/best_model.h5
Epoch 8/101
 - 140s - loss: 0.0014 - metric_fun: 0.5017 - val_loss: 0.0015 - val_metric_fun: 0.5395

Epoch 00008: val_loss did not improve from 0.00146
Epoch 9/101
 - 140s - loss: 0.0013 - metric_fun: 0.5225 - val_loss: 0.0014 - val_metric_fun: 0.4902

Epoch 00009: val_loss improved from 0.00146 to 0.00142, saving model to ./weights/best_model.h5
Epoch 10/101
 - 139s - loss: 0.0012 - metric_fun: 0.5314 - val_loss: 0.0013 - val_metric_fun: 0.5644

Epoch 00010: val_loss improved from 0.00142 to 0.00126, saving model to ./weights/best_model.h5
Epoch 11/101
 - 139s - loss: 0.0011 - metric_fun: 0.5430 - val_loss: 0.0013 - val_metric_fun: 0.5479

Epoch 00011: val_loss did not improve from 0.00126
Epoch 12/101
 - 138s - loss: 0.0011 - metric_fun: 0.5415 - val_loss: 0.0012 - val_metric_fun: 0.5329

Epoch 00012: val_loss improved from 0.00126 to 0.00124, saving model to ./weights/best_model.h5
Epoch 13/101
 - 139s - loss: 0.0011 - metric_fun: 0.5429 - val_loss: 0.0012 - val_metric_fun: 0.5220

Epoch 00013: val_loss improved from 0.00124 to 0.00117, saving model to ./weights/best_model.h5
Epoch 14/101
 - 139s - loss: 0.0010 - metric_fun: 0.5585 - val_loss: 0.0012 - val_metric_fun: 0.5787

Epoch 00014: val_loss did not improve from 0.00117
Epoch 15/101
 - 139s - loss: 0.0010 - metric_fun: 0.5622 - val_loss: 0.0013 - val_metric_fun: 0.5195

Epoch 00015: val_loss did not improve from 0.00117
Epoch 16/101
 - 139s - loss: 0.0010 - metric_fun: 0.5641 - val_loss: 0.0012 - val_metric_fun: 0.5722

Epoch 00016: val_loss did not improve from 0.00117
Epoch 17/101
 - 138s - loss: 9.4168e-04 - metric_fun: 0.5757 - val_loss: 0.0011 - val_metric_fun: 0.5346

Epoch 00017: val_loss improved from 0.00117 to 0.00114, saving model to ./weights/best_model.h5
Epoch 18/101
 - 138s - loss: 9.2567e-04 - metric_fun: 0.5780 - val_loss: 0.0012 - val_metric_fun: 0.5857

Epoch 00018: val_loss did not improve from 0.00114
Epoch 19/101
 - 138s - loss: 9.2175e-04 - metric_fun: 0.5789 - val_loss: 0.0011 - val_metric_fun: 0.5380

Epoch 00019: val_loss improved from 0.00114 to 0.00112, saving model to ./weights/best_model.h5
Epoch 20/101
 - 138s - loss: 8.7025e-04 - metric_fun: 0.5886 - val_loss: 0.0011 - val_metric_fun: 0.6003

Epoch 00020: val_loss improved from 0.00112 to 0.00108, saving model to ./weights/best_model.h5
Epoch 21/101
 - 138s - loss: 8.3662e-04 - metric_fun: 0.5983 - val_loss: 0.0011 - val_metric_fun: 0.5629

Epoch 00021: val_loss improved from 0.00108 to 0.00108, saving model to ./weights/best_model.h5
Epoch 22/101
 - 137s - loss: 8.1681e-04 - metric_fun: 0.6027 - val_loss: 0.0010 - val_metric_fun: 0.5796

Epoch 00022: val_loss improved from 0.00108 to 0.00103, saving model to ./weights/best_model.h5
Epoch 23/101
 - 138s - loss: 7.9719e-04 - metric_fun: 0.6074 - val_loss: 0.0010 - val_metric_fun: 0.5756

Epoch 00023: val_loss did not improve from 0.00103
Epoch 24/101
 - 138s - loss: 7.7129e-04 - metric_fun: 0.6112 - val_loss: 0.0010 - val_metric_fun: 0.6068

Epoch 00024: val_loss did not improve from 0.00103
Epoch 25/101
 - 138s - loss: 7.5055e-04 - metric_fun: 0.6169 - val_loss: 0.0010 - val_metric_fun: 0.6029

Epoch 00025: val_loss improved from 0.00103 to 0.00100, saving model to ./weights/best_model.h5
Epoch 26/101
 - 138s - loss: 7.3641e-04 - metric_fun: 0.6193 - val_loss: 9.7283e-04 - val_metric_fun: 0.6101

Epoch 00026: val_loss improved from 0.00100 to 0.00097, saving model to ./weights/best_model.h5
Epoch 27/101
 - 138s - loss: 7.1409e-04 - metric_fun: 0.6241 - val_loss: 9.6773e-04 - val_metric_fun: 0.6158

Epoch 00027: val_loss improved from 0.00097 to 0.00097, saving model to ./weights/best_model.h5
Epoch 28/101
 - 138s - loss: 7.0560e-04 - metric_fun: 0.6270 - val_loss: 9.6657e-04 - val_metric_fun: 0.6242

Epoch 00028: val_loss improved from 0.00097 to 0.00097, saving model to ./weights/best_model.h5
Epoch 29/101
 - 137s - loss: 6.8089e-04 - metric_fun: 0.6318 - val_loss: 9.3871e-04 - val_metric_fun: 0.6360

Epoch 00029: val_loss improved from 0.00097 to 0.00094, saving model to ./weights/best_model.h5
Epoch 30/101
 - 137s - loss: 6.7739e-04 - metric_fun: 0.6331 - val_loss: 9.3494e-04 - val_metric_fun: 0.6151

Epoch 00030: val_loss improved from 0.00094 to 0.00093, saving model to ./weights/best_model.h5
Epoch 31/101
 - 138s - loss: 6.3979e-04 - metric_fun: 0.6414 - val_loss: 9.2663e-04 - val_metric_fun: 0.6016

Epoch 00031: val_loss improved from 0.00093 to 0.00093, saving model to ./weights/best_model.h5
Epoch 32/101
 - 137s - loss: 6.3631e-04 - metric_fun: 0.6431 - val_loss: 9.5167e-04 - val_metric_fun: 0.5808

Epoch 00032: val_loss did not improve from 0.00093
Epoch 33/101
 - 138s - loss: 6.1084e-04 - metric_fun: 0.6491 - val_loss: 9.4201e-04 - val_metric_fun: 0.6267

Epoch 00033: val_loss did not improve from 0.00093
Epoch 34/101
 - 138s - loss: 6.2346e-04 - metric_fun: 0.6454 - val_loss: 9.0468e-04 - val_metric_fun: 0.6198

Epoch 00034: val_loss improved from 0.00093 to 0.00090, saving model to ./weights/best_model.h5
Epoch 35/101
 - 138s - loss: 6.0426e-04 - metric_fun: 0.6499 - val_loss: 9.7733e-04 - val_metric_fun: 0.6248

Epoch 00035: val_loss did not improve from 0.00090
Epoch 36/101
 - 137s - loss: 0.0028 - metric_fun: 0.1189 - val_loss: 0.0028 - val_metric_fun: 0.2716

Epoch 00036: val_loss did not improve from 0.00090
Epoch 37/101
 - 138s - loss: 0.0012 - metric_fun: 0.5491 - val_loss: 0.0013 - val_metric_fun: 0.6304

Epoch 00037: val_loss did not improve from 0.00090
Epoch 38/101
 - 138s - loss: 7.0609e-04 - metric_fun: 0.6362 - val_loss: 9.5021e-04 - val_metric_fun: 0.6387

Epoch 00038: val_loss did not improve from 0.00090
Epoch 39/101
 - 137s - loss: 6.2371e-04 - metric_fun: 0.6522 - val_loss: 8.8707e-04 - val_metric_fun: 0.6377

Epoch 00039: val_loss improved from 0.00090 to 0.00089, saving model to ./weights/best_model.h5
Epoch 40/101
 - 137s - loss: 5.8530e-04 - metric_fun: 0.6604 - val_loss: 9.0931e-04 - val_metric_fun: 0.6408

Epoch 00040: val_loss did not improve from 0.00089
Epoch 41/101
 - 138s - loss: 5.8449e-04 - metric_fun: 0.6600 - val_loss: 9.2961e-04 - val_metric_fun: 0.6149

Epoch 00041: val_loss did not improve from 0.00089
Epoch 42/101
 - 138s - loss: 5.5391e-04 - metric_fun: 0.6647 - val_loss: 8.8105e-04 - val_metric_fun: 0.6207

Epoch 00042: val_loss improved from 0.00089 to 0.00088, saving model to ./weights/best_model.h5
Epoch 43/101
 - 138s - loss: 5.3919e-04 - metric_fun: 0.6673 - val_loss: 0.0010 - val_metric_fun: 0.4746

Epoch 00043: val_loss did not improve from 0.00088
Epoch 44/101
 - 137s - loss: 5.2823e-04 - metric_fun: 0.6701 - val_loss: 8.9554e-04 - val_metric_fun: 0.6218

Epoch 00044: val_loss did not improve from 0.00088
Epoch 45/101
 - 138s - loss: 5.2060e-04 - metric_fun: 0.6719 - val_loss: 9.1253e-04 - val_metric_fun: 0.6153

Epoch 00045: val_loss did not improve from 0.00088
Epoch 46/101
 - 138s - loss: 5.1264e-04 - metric_fun: 0.6724 - val_loss: 9.0975e-04 - val_metric_fun: 0.6156

Epoch 00046: val_loss did not improve from 0.00088
Epoch 47/101
 - 138s - loss: 5.1360e-04 - metric_fun: 0.6727 - val_loss: 8.9364e-04 - val_metric_fun: 0.6386

Epoch 00047: val_loss did not improve from 0.00088
Epoch 48/101
 - 138s - loss: 4.9918e-04 - metric_fun: 0.6765 - val_loss: 9.9187e-04 - val_metric_fun: 0.5569

Epoch 00048: val_loss did not improve from 0.00088
Epoch 49/101
 - 138s - loss: 4.9452e-04 - metric_fun: 0.6768 - val_loss: 8.9285e-04 - val_metric_fun: 0.6222

Epoch 00049: val_loss did not improve from 0.00088
Epoch 50/101
 - 138s - loss: 4.7994e-04 - metric_fun: 0.6805 - val_loss: 8.6382e-04 - val_metric_fun: 0.6440

Epoch 00050: val_loss improved from 0.00088 to 0.00086, saving model to ./weights/best_model.h5
Epoch 51/101
 - 138s - loss: 4.7184e-04 - metric_fun: 0.6832 - val_loss: 8.8753e-04 - val_metric_fun: 0.6373

Epoch 00051: ReduceLROnPlateau reducing learning rate to 0.0010000000474974513.

Epoch 00051: val_loss did not improve from 0.00086
Epoch 52/101
 - 138s - loss: 4.2296e-04 - metric_fun: 0.6932 - val_loss: 8.5459e-04 - val_metric_fun: 0.6471

Epoch 00052: val_loss improved from 0.00086 to 0.00085, saving model to ./weights/best_model.h5
Epoch 53/101
 - 139s - loss: 4.0240e-04 - metric_fun: 0.7000 - val_loss: 8.4070e-04 - val_metric_fun: 0.6459

Epoch 00053: val_loss improved from 0.00085 to 0.00084, saving model to ./weights/best_model.h5
Epoch 54/101
 - 138s - loss: 3.9034e-04 - metric_fun: 0.7032 - val_loss: 8.5440e-04 - val_metric_fun: 0.6480

Epoch 00054: val_loss did not improve from 0.00084
Epoch 55/101
 - 138s - loss: 3.8194e-04 - metric_fun: 0.7056 - val_loss: 8.4945e-04 - val_metric_fun: 0.6532

Epoch 00055: val_loss did not improve from 0.00084
Epoch 56/101
 - 138s - loss: 3.7957e-04 - metric_fun: 0.7070 - val_loss: 8.5744e-04 - val_metric_fun: 0.6562

Epoch 00056: val_loss did not improve from 0.00084
Epoch 57/101
 - 138s - loss: 3.7035e-04 - metric_fun: 0.7085 - val_loss: 8.5147e-04 - val_metric_fun: 0.6581

Epoch 00057: val_loss did not improve from 0.00084
Epoch 58/101
 - 138s - loss: 3.7032e-04 - metric_fun: 0.7087 - val_loss: 8.8730e-04 - val_metric_fun: 0.6435

Epoch 00058: val_loss did not improve from 0.00084
Epoch 59/101
 - 137s - loss: 3.6734e-04 - metric_fun: 0.7096 - val_loss: 8.6049e-04 - val_metric_fun: 0.6537

Epoch 00059: val_loss did not improve from 0.00084
Epoch 60/101
 - 137s - loss: 3.5777e-04 - metric_fun: 0.7131 - val_loss: 8.5010e-04 - val_metric_fun: 0.6544

Epoch 00060: val_loss did not improve from 0.00084
Epoch 61/101
 - 138s - loss: 3.5446e-04 - metric_fun: 0.7138 - val_loss: 8.4949e-04 - val_metric_fun: 0.6475

Epoch 00061: val_loss did not improve from 0.00084
Epoch 62/101
 - 137s - loss: 3.5406e-04 - metric_fun: 0.7140 - val_loss: 8.4650e-04 - val_metric_fun: 0.6453

Epoch 00062: val_loss did not improve from 0.00084
Epoch 63/101
 - 138s - loss: 3.4524e-04 - metric_fun: 0.7170 - val_loss: 8.9202e-04 - val_metric_fun: 0.6503

Epoch 00063: val_loss did not improve from 0.00084
Epoch 64/101
 - 138s - loss: 3.4110e-04 - metric_fun: 0.7176 - val_loss: 8.8055e-04 - val_metric_fun: 0.6517

Epoch 00064: val_loss did not improve from 0.00084
Epoch 65/101
 - 138s - loss: 3.3773e-04 - metric_fun: 0.7186 - val_loss: 8.6781e-04 - val_metric_fun: 0.6541

Epoch 00065: val_loss did not improve from 0.00084
Epoch 66/101
 - 138s - loss: 3.3641e-04 - metric_fun: 0.7188 - val_loss: 8.9113e-04 - val_metric_fun: 0.6533

Epoch 00066: val_loss did not improve from 0.00084
Epoch 67/101
 - 138s - loss: 3.3434e-04 - metric_fun: 0.7192 - val_loss: 8.5664e-04 - val_metric_fun: 0.6495

Epoch 00067: val_loss did not improve from 0.00084
Epoch 68/101
 - 138s - loss: 3.2869e-04 - metric_fun: 0.7212 - val_loss: 8.5608e-04 - val_metric_fun: 0.6539

Epoch 00068: val_loss did not improve from 0.00084
Epoch 69/101
 - 139s - loss: 3.2503e-04 - metric_fun: 0.7216 - val_loss: 8.7552e-04 - val_metric_fun: 0.6495

Epoch 00069: val_loss did not improve from 0.00084
Epoch 70/101
 - 139s - loss: 3.2048e-04 - metric_fun: 0.7238 - val_loss: 8.6601e-04 - val_metric_fun: 0.6570

Epoch 00070: val_loss did not improve from 0.00084
Epoch 71/101
 - 138s - loss: 3.1892e-04 - metric_fun: 0.7233 - val_loss: 8.6785e-04 - val_metric_fun: 0.6555

Epoch 00071: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.

Epoch 00071: val_loss did not improve from 0.00084
Epoch 72/101
 - 138s - loss: 2.9887e-04 - metric_fun: 0.7290 - val_loss: 8.5505e-04 - val_metric_fun: 0.6614

Epoch 00072: val_loss did not improve from 0.00084
Epoch 73/101
 - 137s - loss: 2.8909e-04 - metric_fun: 0.7330 - val_loss: 8.6064e-04 - val_metric_fun: 0.6549

Epoch 00073: val_loss did not improve from 0.00084
Epoch 74/101
 - 137s - loss: 2.8498e-04 - metric_fun: 0.7343 - val_loss: 8.5015e-04 - val_metric_fun: 0.6618

Epoch 00074: val_loss did not improve from 0.00084
Epoch 75/101
 - 138s - loss: 2.7990e-04 - metric_fun: 0.7358 - val_loss: 8.6105e-04 - val_metric_fun: 0.6600

Epoch 00075: val_loss did not improve from 0.00084
Epoch 76/101
 - 137s - loss: 2.7847e-04 - metric_fun: 0.7363 - val_loss: 8.6555e-04 - val_metric_fun: 0.6617

Epoch 00076: val_loss did not improve from 0.00084
Epoch 77/101
 - 138s - loss: 2.7400e-04 - metric_fun: 0.7382 - val_loss: 8.5774e-04 - val_metric_fun: 0.6634

Epoch 00077: val_loss did not improve from 0.00084
Epoch 78/101
 - 139s - loss: 2.7335e-04 - metric_fun: 0.7377 - val_loss: 8.6121e-04 - val_metric_fun: 0.6663

Epoch 00078: val_loss did not improve from 0.00084
Epoch 79/101
 - 138s - loss: 2.7048e-04 - metric_fun: 0.7397 - val_loss: 8.6590e-04 - val_metric_fun: 0.6654

Epoch 00079: val_loss did not improve from 0.00084
Epoch 80/101
 - 137s - loss: 2.7054e-04 - metric_fun: 0.7384 - val_loss: 8.6151e-04 - val_metric_fun: 0.6687

Epoch 00080: val_loss did not improve from 0.00084
Epoch 81/101
 - 137s - loss: 2.6655e-04 - metric_fun: 0.7398 - val_loss: 8.7401e-04 - val_metric_fun: 0.6643

Epoch 00081: val_loss did not improve from 0.00084
Epoch 82/101
 - 137s - loss: 2.6539e-04 - metric_fun: 0.7405 - val_loss: 8.6976e-04 - val_metric_fun: 0.6674

Epoch 00082: val_loss did not improve from 0.00084
Epoch 83/101
 - 137s - loss: 2.6397e-04 - metric_fun: 0.7411 - val_loss: 8.7259e-04 - val_metric_fun: 0.6628

Epoch 00083: val_loss did not improve from 0.00084
Epoch 84/101
 - 137s - loss: 2.6117e-04 - metric_fun: 0.7416 - val_loss: 8.8017e-04 - val_metric_fun: 0.6659

Epoch 00084: val_loss did not improve from 0.00084
Epoch 85/101
 - 137s - loss: 2.6039e-04 - metric_fun: 0.7422 - val_loss: 8.8418e-04 - val_metric_fun: 0.6595

Epoch 00085: val_loss did not improve from 0.00084
Epoch 86/101
 - 137s - loss: 2.5768e-04 - metric_fun: 0.7428 - val_loss: 8.8192e-04 - val_metric_fun: 0.6586

Epoch 00086: val_loss did not improve from 0.00084
Epoch 87/101
 - 137s - loss: 2.5782e-04 - metric_fun: 0.7426 - val_loss: 8.7310e-04 - val_metric_fun: 0.6659

Epoch 00087: val_loss did not improve from 0.00084
Epoch 88/101
 - 137s - loss: 2.5440e-04 - metric_fun: 0.7435 - val_loss: 8.7222e-04 - val_metric_fun: 0.6649

Epoch 00088: val_loss did not improve from 0.00084
Epoch 89/101
 - 137s - loss: 2.5220e-04 - metric_fun: 0.7446 - val_loss: 8.7518e-04 - val_metric_fun: 0.6630

Epoch 00089: val_loss did not improve from 0.00084
Epoch 90/101
 - 137s - loss: 2.5134e-04 - metric_fun: 0.7443 - val_loss: 8.8275e-04 - val_metric_fun: 0.6609

Epoch 00090: val_loss did not improve from 0.00084
Epoch 91/101
 - 137s - loss: 2.5186e-04 - metric_fun: 0.7443 - val_loss: 8.7585e-04 - val_metric_fun: 0.6635

Epoch 00091: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.

Epoch 00091: val_loss did not improve from 0.00084
Epoch 92/101
 - 137s - loss: 2.4157e-04 - metric_fun: 0.7470 - val_loss: 8.8022e-04 - val_metric_fun: 0.6668

Epoch 00092: val_loss did not improve from 0.00084
Epoch 93/101
 - 137s - loss: 2.3681e-04 - metric_fun: 0.7486 - val_loss: 8.7852e-04 - val_metric_fun: 0.6686

Epoch 00093: val_loss did not improve from 0.00084
Epoch 94/101
 - 138s - loss: 2.3373e-04 - metric_fun: 0.7492 - val_loss: 8.8353e-04 - val_metric_fun: 0.6692

Epoch 00094: val_loss did not improve from 0.00084
Epoch 95/101
 - 137s - loss: 2.3404e-04 - metric_fun: 0.7495 - val_loss: 8.8067e-04 - val_metric_fun: 0.6661

Epoch 00095: val_loss did not improve from 0.00084
Epoch 96/101
 - 137s - loss: 2.3258e-04 - metric_fun: 0.7504 - val_loss: 8.8449e-04 - val_metric_fun: 0.6669

Epoch 00096: val_loss did not improve from 0.00084
Epoch 97/101
 - 137s - loss: 2.3093e-04 - metric_fun: 0.7499 - val_loss: 8.7876e-04 - val_metric_fun: 0.6699

Epoch 00097: val_loss did not improve from 0.00084
Epoch 98/101
 - 137s - loss: 2.3011e-04 - metric_fun: 0.7510 - val_loss: 8.7794e-04 - val_metric_fun: 0.6692

Epoch 00098: val_loss did not improve from 0.00084
Epoch 99/101
 - 137s - loss: 2.2807e-04 - metric_fun: 0.7511 - val_loss: 8.8605e-04 - val_metric_fun: 0.6662

Epoch 00099: val_loss did not improve from 0.00084
Epoch 100/101
 - 137s - loss: 2.2720e-04 - metric_fun: 0.7518 - val_loss: 8.8090e-04 - val_metric_fun: 0.6675

Epoch 00100: val_loss did not improve from 0.00084
Epoch 101/101
 - 137s - loss: 2.2722e-04 - metric_fun: 0.7519 - val_loss: 8.8391e-04 - val_metric_fun: 0.6673

Epoch 00101: val_loss did not improve from 0.00084





the accuracy of test data is: 97.50%